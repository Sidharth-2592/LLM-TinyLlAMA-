{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiE1bgddCcShYRRBf9dKqx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidharth-2592/LLM-TinyLlAMA-/blob/main/LLM_for_Q%26A_(TinyLlama).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJqJJFevRwQh",
        "outputId": "8753490d-8d70-4365-e1fa-01941d2b0e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install -q transformers accelerate bitsandbytes sentencepiece\n"
      ],
      "metadata": {
        "id": "HH8ZTOzDT3k9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "o7kQ7jG1SUun"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def setup_llama3_model():\n",
        "    \"\"\"\n",
        "    Load an open-source language model that's available without authentication\n",
        "    \"\"\"\n",
        "    # Use TinyLlama, which is open-source and doesn't require authentication\n",
        "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "    # Display loading message\n",
        "    print(f\"Loading {model_id} model (this may take a minute)...\")\n",
        "\n",
        "    # Load tokenizer without requiring token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # Load model with 8-bit quantization to reduce memory usage\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_8bit=True\n",
        "    )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def create_qa_system(model, tokenizer):\n",
        "    \"\"\"\n",
        "    Create a question-answering pipeline using the loaded model\n",
        "    \"\"\"\n",
        "    # Create Q&A pipeline\n",
        "    qa_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    return qa_pipeline\n",
        "\n",
        "def format_prompt(question):\n",
        "    \"\"\"\n",
        "    Format the prompt for TinyLlama chat model\n",
        "    \"\"\"\n",
        "    prompt = f\"<|user|>\\n{question}\\n<|assistant|>\"\n",
        "    return prompt\n",
        "\n",
        "def answer_question(qa_pipeline, question):\n",
        "    \"\"\"\n",
        "    Generate an answer to the given question\n",
        "    \"\"\"\n",
        "    prompt = format_prompt(question)\n",
        "\n",
        "    # Generate response\n",
        "    response = qa_pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "    # Extract only the model's answer from the response\n",
        "    answer = response.split(\"<|assistant|>\")[1].strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Alternative: If you want to use your own Hugging Face token\n",
        "def setup_with_token(token):\n",
        "    \"\"\"\n",
        "    Alternative setup function that uses a provided token\n",
        "    \"\"\"\n",
        "    from huggingface_hub import login\n",
        "    login(token)\n",
        "\n",
        "    # Now you can use Meta-Llama-3 models\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "    print(f\"Loading {model_id} model (this may take a minute)...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        load_in_8bit=True\n",
        "    )\n",
        "\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D97rRvXSm4g",
        "outputId": "3cca4202-215c-4335-af01-31131e1dc8f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Option 1: Use TinyLlama without authentication\n",
        "    model, tokenizer = setup_llama3_model()\n",
        "\n",
        "    # Option 2: If you have a token, uncomment below and replace YOUR_TOKEN\n",
        "    # model, tokenizer = setup_with_token(\"YOUR_TOKEN\")\n",
        "\n",
        "    # Create the QA system\n",
        "    qa_system = create_qa_system(model, tokenizer)\n",
        "\n",
        "    # Interactive Q&A loop\n",
        "    print(\"\\n===== LLM Q&A System =====\")\n",
        "    print(\"Type 'exit' to quit the program\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \")\n",
        "        if question.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        print(\"\\nThinking...\")\n",
        "        answer = answer_question(qa_system, question)\n",
        "        print(f\"\\nAnswer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NXsq3fVUAlT",
        "outputId": "f777ab35-55db-476f-b36b-72603fb8d70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0 model (this may take a minute)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== LLM Q&A System =====\n",
            "Type 'exit' to quit the program\n",
            "\n",
            "Your question: Who is Narendra Modi?\n",
            "\n",
            "Thinking...\n",
            "\n",
            "Answer: Narendra Modi is the current Prime Minister of India. He was sworn in as the 14th Prime Minister of India on 26 May 2014, after winning the 2014 Indian general election. Modi is a Bharatiya Janata Party (BJP) leader, and he was the chief minister of Gujarat from 2001 to 2014. He is known for his strong leadership, economic reforms, and focus on improving governance and infrastructure development.\n"
          ]
        }
      ]
    }
  ]
}